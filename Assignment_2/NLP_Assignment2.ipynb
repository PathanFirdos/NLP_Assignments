{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO_G2Ziw0zht",
        "outputId": "73fdbf5b-b8ba-46ad-9067-b05fe36afaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Natural language processing is fun\",\n",
        "    \"I love machine learning\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "aPFo5AR-07rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(count_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag-of-Words (Count Occurrence):\")\n",
        "print(bow_counts.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-YSOpBI09F2",
        "outputId": "53df9a89-9bec-432b-eee3-025b215acea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['fun' 'is' 'language' 'learning' 'love' 'machine' 'natural' 'processing']\n",
            "\n",
            "Bag-of-Words (Count Occurrence):\n",
            "[[0 0 1 0 1 0 1 1]\n",
            " [1 1 1 0 0 0 1 1]\n",
            " [0 0 0 1 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_array = bow_counts.toarray()\n",
        "normalized_bow = bow_array / bow_array.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(\"\\nNormalized Bag-of-Words:\")\n",
        "print(normalized_bow)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdznYnGI1Q_n",
        "outputId": "b7ed2f0c-495e-48a0-e92f-3905282426f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Bag-of-Words:\n",
            "[[0.         0.         0.25       0.         0.25       0.\n",
            "  0.25       0.25      ]\n",
            " [0.2        0.2        0.2        0.         0.         0.\n",
            "  0.2        0.2       ]\n",
            " [0.         0.         0.         0.33333333 0.33333333 0.33333333\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"\\nTF-IDF Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgpNzLE91WOK",
        "outputId": "b39c03cd-d3d2-4708-c614-983668e0120d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Vocabulary:\n",
            "['fun' 'is' 'language' 'learning' 'love' 'machine' 'natural' 'processing']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.5        0.         0.5        0.\n",
            "  0.5        0.5       ]\n",
            " [0.51741994 0.51741994 0.3935112  0.         0.         0.\n",
            "  0.3935112  0.3935112 ]\n",
            " [0.         0.         0.         0.62276601 0.4736296  0.62276601\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Natural language processing is fun\",\n",
        "    \"I love machine learning\"\n",
        "]\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "vgeh1Kxa1YFN",
        "outputId": "6418a627-492b-4d95-d105-4dab0b612e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-937149779.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
          ]
        }
      ]
    }
  ]
}